{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "initial_id",
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from __future__ import annotations\n",
    "\n",
    "import logging\n",
    "import re\n",
    "from dataclasses import dataclass\n",
    "from typing import Dict, List, Optional, Tuple\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from transformers import (\n",
    "    AutoModel,\n",
    "    AutoModelForCausalLM,\n",
    "    AutoTokenizer,\n",
    "    GenerationConfig,\n",
    "    PreTrainedTokenizerBase,\n",
    ")\n",
    "\n",
    "# Optional vLLM backend -----------------------------------------------------\n",
    "try:\n",
    "    from vllm import LLM, SamplingParams  # type: ignore\n",
    "    _VLLM_AVAILABLE = True\n",
    "except ImportError:  # pragma: no cover\n",
    "    _VLLM_AVAILABLE = False\n",
    "\n",
    "# ---------------------------------------------------------------------------\n",
    "# Logging / constants\n",
    "# ---------------------------------------------------------------------------\n",
    "logging.basicConfig(level=logging.INFO, format=\"[%(levelname)s] %(message)s\")\n",
    "logger = logging.getLogger(\"ensemble_inference\")\n",
    "\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "EOS_TEXT = \"\"  # Most Qwen / Llama models use empty string as EOS\n",
    "STEP_TOKEN = \"<extra_0>\"  # Token separator used by reward model\n",
    "SYSTEM_PROMPT = \"You are a helpful assistant.\"\n",
    "STOP_TOKENS_TEXT = {\".\", \"\\n\"}  # Stop decoding after these tokens\n",
    "\n",
    "# ---------------------------------------------------------------------------\n",
    "# Conversation Template\n",
    "# ---------------------------------------------------------------------------\n",
    "\n",
    "class ConversationTemplate:\n",
    "    \"\"\"\n",
    "    A conversation template for constructing dialogue prompts.\n",
    "    It includes a system prompt, a single user question, and accumulated assistant responses.\n",
    "    \"\"\"\n",
    "    def __init__(self, system_prompt: str, initial_question: str):\n",
    "        self.system = system_prompt\n",
    "        self.question = initial_question\n",
    "        self.assistant_parts: List[str] = []  # Collected assistant responses\n",
    "\n",
    "    def add_assistant(self, content: str):\n",
    "        \"\"\"Append a new assistant response to the prompt context.\"\"\"\n",
    "        self.assistant_parts.append(content.strip())\n",
    "\n",
    "    def render(self) -> str:\n",
    "        \"\"\"\n",
    "        Render the full prompt to be fed into a language model.\n",
    "        It includes the system message, user input, and accumulated assistant responses.\n",
    "        \"\"\"\n",
    "        lines = [\n",
    "            f\"[SYSTEM] {self.system} [/SYSTEM]\",\n",
    "            f\"<user>\\n{self.question.strip()}\\n</user>\",\n",
    "            f\"<assistant>\\n\" + \"\\n\".join(self.assistant_parts)\n",
    "        ]\n",
    "        return \"\".join(lines)\n",
    "\n",
    "# ---------------------------------------------------------------------------\n",
    "# Utility: trim text at the last occurrence of stop tokens\n",
    "# ---------------------------------------------------------------------------\n",
    "\n",
    "def _trim_text(txt: str) -> str:\n",
    "    \"\"\"Truncate the text after the last known stop token for cleaner outputs.\"\"\"\n",
    "    best_pos = -1\n",
    "    best_tok = None\n",
    "    for tok in STOP_TOKENS_TEXT:\n",
    "        pos = txt.rfind(tok)\n",
    "        if pos > best_pos:\n",
    "            best_pos = pos\n",
    "            best_tok = tok\n",
    "    if best_pos != -1:\n",
    "        return txt[: best_pos + len(best_tok)]\n",
    "    return txt\n",
    "\n",
    "# ---------------------------------------------------------------------------\n",
    "# Utility: extract token-level reward scores from logits\n",
    "# ---------------------------------------------------------------------------\n",
    "\n",
    "def _step_rewards(logits: torch.Tensor, mask: torch.Tensor):\n",
    "    \"\"\"\n",
    "    Compute step-wise probabilities using softmax over logits.\n",
    "    Only consider positions where mask is non-zero (STEP_TOKEN positions).\n",
    "    \"\"\"\n",
    "    probs = F.softmax(logits, dim=-1) * mask.unsqueeze(-1)\n",
    "    arr: List[List[float]] = []\n",
    "    for sample in probs:\n",
    "        pos = sample[sample != 0].view(-1, 2)[:, 1]\n",
    "        arr.append(pos.cpu().tolist())\n",
    "    return arr\n",
    "\n",
    "# ---------------------------------------------------------------------------\n",
    "# Output container for model generation\n",
    "# ---------------------------------------------------------------------------\n",
    "\n",
    "@dataclass\n",
    "class GenOutput:\n",
    "    text: str\n",
    "    ended_with_eos: bool  # Whether EOS token was generated\n",
    "\n",
    "# ---------------------------------------------------------------------------\n",
    "# Abstract base class for any generator (HF or vLLM)\n",
    "# ---------------------------------------------------------------------------\n",
    "\n",
    "class BaseGenerator:\n",
    "    name: str\n",
    "\n",
    "    def generate(self, prompt: str, **kw) -> GenOutput:\n",
    "        \"\"\"Abstract method for generating model outputs.\"\"\"\n",
    "        raise NotImplementedError\n",
    "\n",
    "# ---------------------------------------------------------------------------\n",
    "# HuggingFace Transformers-based Generator\n",
    "# ---------------------------------------------------------------------------\n",
    "\n",
    "class HFGenerator(BaseGenerator):\n",
    "    def __init__(self, path: str, *, device: str = \"auto\", dtype: torch.dtype = torch.bfloat16):\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(path, trust_remote_code=True)\n",
    "        self.model = AutoModelForCausalLM.from_pretrained(\n",
    "            path,\n",
    "            torch_dtype=dtype,\n",
    "            device_map=device,\n",
    "            trust_remote_code=True\n",
    "        ).eval()\n",
    "        self.name = path\n",
    "        self.device = next(self.model.parameters()).device if device == \"auto\" else torch.device(device)\n",
    "\n",
    "        # Optional stop string list\n",
    "        self.stop_strings = list(STOP_TOKENS_TEXT) + [\n",
    "            self.tokenizer.decode([self.tokenizer.eos_token_id], skip_special_tokens=False)\n",
    "        ]\n",
    "\n",
    "    @torch.inference_mode()\n",
    "    def generate(self, prompt: str, *, max_tokens=64, temperature=0.95, top_p=0.7) -> GenOutput:\n",
    "        ids = self.tokenizer(prompt, return_tensors=\"pt\").to(self.device)\n",
    "        cfg = GenerationConfig(\n",
    "            do_sample=True,\n",
    "            temperature=temperature,\n",
    "            top_p=top_p,\n",
    "            max_new_tokens=max_tokens,\n",
    "            pad_token_id=self.tokenizer.eos_token_id,\n",
    "        )\n",
    "        out = self.model.generate(**ids, generation_config=cfg, tokenizer=self.tokenizer)[0]\n",
    "        ended = bool(self.tokenizer.eos_token_id in out)\n",
    "        txt = self.tokenizer.decode(out[len(ids[\"input_ids\"][0]):], skip_special_tokens=False)\n",
    "        return GenOutput(_trim_text(txt) if not ended else txt, ended)\n",
    "\n",
    "# ---------------------------------------------------------------------------\n",
    "# vLLM-based Generator\n",
    "# ---------------------------------------------------------------------------\n",
    "\n",
    "class VLLMGenerator(BaseGenerator):\n",
    "    def __init__(self, path: str):\n",
    "        if not _VLLM_AVAILABLE:\n",
    "            raise RuntimeError(\"vLLM is not installed.\")\n",
    "        self._llm = LLM(model=path)\n",
    "        self._sp = SamplingParams(max_tokens=128, temperature=0.95, top_p=0.7, stop=list(STOP_TOKENS_TEXT))\n",
    "        self.name = path\n",
    "        self._eos_text = EOS_TEXT\n",
    "\n",
    "    @torch.inference_mode()\n",
    "    def generate(self, prompt: str, *, max_tokens=30, temperature=0.95, top_p=0.7) -> GenOutput:\n",
    "        self._sp.max_tokens, self._sp.temperature, self._sp.top_p = max_tokens, temperature, top_p\n",
    "        txt = self._llm.generate([prompt], self._sp)[0].outputs[0].text\n",
    "        ended = txt.endswith(self._eos_text)\n",
    "        return GenOutput(_trim_text(txt), ended)\n",
    "\n",
    "# ---------------------------------------------------------------------------\n",
    "# ModelPool: caches all loaded generators and reward models\n",
    "# ---------------------------------------------------------------------------\n",
    "\n",
    "class ModelPool:\n",
    "    _gen_cache: Dict[Tuple[str, str], BaseGenerator] = {}\n",
    "    _reward_cache: Dict[str, \"PRMScorer\"] = {}\n",
    "\n",
    "    @classmethod\n",
    "    def get_generator(cls, path: str, engine: str = \"hf\") -> BaseGenerator:\n",
    "        key = (engine, path)\n",
    "        if key not in cls._gen_cache:\n",
    "            logger.info(\"[Pool] loading %s (%s)\", path, engine)\n",
    "            cls._gen_cache[key] = HFGenerator(path) if engine == \"hf\" else VLLMGenerator(path)\n",
    "        return cls._gen_cache[key]\n",
    "\n",
    "    @classmethod\n",
    "    def get_reward(cls, path: str) -> \"PRMScorer\":\n",
    "        if path not in cls._reward_cache:\n",
    "            logger.info(\"[Pool] loading reward model %s\", path)\n",
    "            cls._reward_cache[path] = PRMScorer(path)\n",
    "        return cls._reward_cache[path]\n",
    "\n",
    "# ---------------------------------------------------------------------------\n",
    "# PRMScorer: reward model used for evaluating step-level outputs\n",
    "# ---------------------------------------------------------------------------\n",
    "\n",
    "class PRMScorer:\n",
    "    def __init__(self, path: str):\n",
    "        self.tok = AutoTokenizer.from_pretrained(path, trust_remote_code=True)\n",
    "        self.mod = AutoModel.from_pretrained(path, torch_dtype=torch.bfloat16, device_map=\"auto\", trust_remote_code=True).eval()\n",
    "        self.sep_id = self.tok.encode(STEP_TOKEN)[0]\n",
    "\n",
    "    @torch.inference_mode()\n",
    "    def score(self, question: str, answer: str) -> float:\n",
    "        \"\"\"Compute reward score from model output at STEP_TOKEN positions.\"\"\"\n",
    "        if not answer.endswith(STEP_TOKEN):\n",
    "            answer += STEP_TOKEN\n",
    "        msgs = [\n",
    "            {\"role\": \"system\", \"content\": SYSTEM_PROMPT},\n",
    "            {\"role\": \"user\", \"content\": question},\n",
    "            {\"role\": \"assistant\", \"content\": answer},\n",
    "        ]\n",
    "        convo = self.tok.apply_chat_template(msgs, tokenize=False, add_generation_prompt=False)\n",
    "        ids = self.tok(convo, return_tensors=\"pt\").input_ids.to(DEVICE)\n",
    "        mask = ids == self.sep_id\n",
    "        probs = _step_rewards(self.mod(ids).logits, mask)[0]\n",
    "        return float(sum(probs) / len(probs) * 10.0) if probs else 0.0\n",
    "\n",
    "# ---------------------------------------------------------------------------\n",
    "# EnsembleReasoner: multi-model decoding loop with step-wise reward scoring\n",
    "# ---------------------------------------------------------------------------\n",
    "\n",
    "class EnsembleReasoner:\n",
    "    def __init__(self, generators: List[BaseGenerator], scorer: PRMScorer, max_rounds: int = 5,\n",
    "                 score_threshold: float = 0.5, accumulate_context: bool = True):\n",
    "        self.generators = generators\n",
    "        self.scorer = scorer\n",
    "        self.max_rounds = max_rounds\n",
    "        self.score_threshold = score_threshold\n",
    "        self.accumulate_context = accumulate_context\n",
    "\n",
    "    def __call__(self, question: str) -> str:\n",
    "        \"\"\"\n",
    "        Iteratively decode using multiple generators.\n",
    "        In each round, the best candidate (with highest reward) is selected and appended.\n",
    "        Generation stops early if reward is low or EOS is emitted.\n",
    "        \"\"\"\n",
    "        convo = ConversationTemplate(SYSTEM_PROMPT, question)\n",
    "\n",
    "        for rnd in range(1, self.max_rounds + 1):\n",
    "            prompt = convo.render()\n",
    "\n",
    "            # Filter out generators that exceed input length\n",
    "            available_gens: List[BaseGenerator] = []\n",
    "            for g in self.generators:\n",
    "                tok = getattr(g, \"tokenizer\", None)\n",
    "                if tok is not None:\n",
    "                    length = tok(prompt, return_tensors=\"pt\").input_ids.size(1)\n",
    "                    if length > tok.model_max_length:\n",
    "                        logger.info(\"Skip %s: prompt length %d > max %d\",\n",
    "                                    g.name, length, tok.model_max_length)\n",
    "                        continue\n",
    "                available_gens.append(g)\n",
    "\n",
    "            if not available_gens:\n",
    "                logger.error(\"No generators available for current prompt length; stopping early.\")\n",
    "                break\n",
    "\n",
    "            outs = [g.generate(prompt) for g in available_gens]\n",
    "            segs = [o.text for o in outs]\n",
    "\n",
    "            # Score each candidate using prompt + STEP_TOKEN + candidate + STEP_TOKEN\n",
    "            scores = []\n",
    "            for o in outs:\n",
    "                augmented = prompt + STEP_TOKEN + o.text + STEP_TOKEN\n",
    "                scores.append(self.scorer.score(question, augmented))\n",
    "\n",
    "            for g, t, s in zip(available_gens, segs, scores):\n",
    "                logger.info(\"→ %s | %.2f | %s\", g.name, s, t.replace(\"\\n\", \"\\\\n\"))\n",
    "\n",
    "            best_idx = int(torch.tensor(scores).argmax())\n",
    "            best_out = outs[best_idx]\n",
    "            best_score = scores[best_idx]\n",
    "\n",
    "            if best_score < self.score_threshold:\n",
    "                logger.info(\"Stop: best score %.2f < threshold\", best_score)\n",
    "                continue\n",
    "\n",
    "            convo.add_assistant(best_out.text)\n",
    "\n",
    "            if best_out.ended_with_eos:\n",
    "                logger.info(\"Early stop: EOS token emitted\")\n",
    "                break\n",
    "\n",
    "        # Return the final composed assistant response\n",
    "        return \"\\n\".join(convo.assistant_parts)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
